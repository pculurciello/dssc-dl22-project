{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, layers_data: list, learning_rate=0.01, optimizer=optim.Adam):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_size = input_size  # Can be useful later ...\n",
    "        for size, activation in layers_data:\n",
    "            self.layers.append(nn.Linear(input_size, size))\n",
    "            input_size = size  # For the next layer\n",
    "            if activation is not None:\n",
    "                assert isinstance(activation, Module), \\\n",
    "                    \"Each tuples should contain a size (int) and a torch.nn.modules.Module.\"\n",
    "                self.layers.append(activation)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer(params=self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer(input_data)\n",
    "        return input_data\n",
    "\n",
    "\n",
    "# test that the net is working properly \n",
    "if __name__ == \"__main__\":\n",
    "    data_size = 5\n",
    "    layer1, layer2 = 10, 10\n",
    "    output_size = 2\n",
    "    data = torch.randn(data_size)\n",
    "    mlp = Model(data_size, [(layer1, nn.ReLU()), (layer2, nn.ReLU()), (output_size, nn.Sigmoid())])\n",
    "    output = mlp(data)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66257c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1943bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #shape of the tensor input to the first conv layer\n",
    "        self.input_shape = input_shape # [1, n_mfcc, n_samples]\n",
    "        #shape of the tensor output of the last conv layer\n",
    "        self.output_shape = []\n",
    "        self.conv_filters = conv_filters # [2, 4, 8]\n",
    "        self.conv_transpose_filters = conv_filters[:-1][::-1]\n",
    "        self.conv_transpose_filters.append(1)\n",
    "        self.conv_kernels = conv_kernels # [3, 5, 3]\n",
    "        self.conv_strides = conv_strides # [1, 2, 2]\n",
    "        self.shape_before_bottleneck = None\n",
    "        self.latent_space_dim = latent_space_dim # 2\n",
    "        self.reconstruction_loss_weight = 1000\n",
    "        self._num_conv_layers = len(conv_filters)\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.layer2id = dict()\n",
    "        self.layer2id['encoder'] = dict()\n",
    "        self.layer2id['decoder'] = dict()   \n",
    "     \n",
    "        self._compute_output_shape()\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        \n",
    "    def _build_encoder(self):\n",
    "        self._add_conv_layers()\n",
    "        self._add_bottleneck()\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        self._invert_bottleneck()\n",
    "        self._add_conv_transpose_layers()\n",
    "        \n",
    "    def _add_conv_layers(self):\n",
    "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
    "        for block_index in range(self._num_conv_layers):\n",
    "            block_number = block_index + 1\n",
    "            layer_index = 0\n",
    "            self.layer2id['encoder']['conv{}'.format(block_number)] = 3 * block_index + layer_index \n",
    "            self.encoder.add_module(\n",
    "                'conv{}'.format(block_number),\n",
    "                self._add_conv_layer(block_index))\n",
    "            layer_index += 1\n",
    "            self.layer2id['encoder']['relu{}'.format(block_number)] = 3 * block_index + layer_index \n",
    "            self.encoder.add_module(\n",
    "                'relu{}'.format(block_number),\n",
    "                nn.ReLU())\n",
    "            layer_index += 1\n",
    "            self.layer2id['encoder']['batchnorm{}'.format(block_number)] = 3 * block_index + layer_index \n",
    "            self.encoder.add_module(\n",
    "                'batchnorm{}'.format(block_number),\n",
    "                nn.BatchNorm2d(self.conv_filters[block_index]))\n",
    "            \n",
    "    def _add_conv_layer(self, block_index):\n",
    "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
    "        conv 2d + ReLU + batch normalization.\n",
    "        \"\"\"\n",
    "        out_channels = self.conv_filters\n",
    "        in_channels = []\n",
    "        in_channels.append(self.input_shape[0])\n",
    "\n",
    "        for i in range(len(self.conv_filters) - 1):\n",
    "            in_channels.append(self.conv_filters[i])\n",
    "            \n",
    "        #print('conv in channels ', in_channels)\n",
    "        #print('conv out channels ', out_channels)\n",
    "            \n",
    "        conv_layer = nn.Conv2d(\n",
    "            in_channels[block_index],\n",
    "            out_channels[block_index],\n",
    "            self.conv_kernels[block_index],\n",
    "            stride=self.conv_strides[block_index],\n",
    "            padding=1\n",
    "        )\n",
    "        return conv_layer\n",
    "    \n",
    "    def _compute_output_shape(self):\n",
    "        #convolution output shape is computed with this formula (for each dimension): [(W−K+2P)/S]+1\n",
    "        #W is the input volume - in your case 128\n",
    "        #K is the Kernel size - in your case 5\n",
    "        #P is the padding - in your case 0 i believe\n",
    "        #S is the stride - which you have not provided.\n",
    "        inp = self.input_shape\n",
    "        for k in range(len(self.conv_kernels)):\n",
    "            K = self.conv_kernels[k]\n",
    "            S = self.conv_strides[k]\n",
    "            P = 1\n",
    "            out = []\n",
    "            out.append(self.conv_filters[k])\n",
    "            for w in range(len(inp) -1):\n",
    "                W = inp[w + 1]\n",
    "                out.append(int(((W - K + (2 * P)) / S) + 1))\n",
    "            inp = out\n",
    "            self.output_shape.append(out)\n",
    "    \n",
    "    def _add_conv_transpose_layers(self):\n",
    "        \"\"\"Create all convolutional blocks in decoder.\"\"\"\n",
    "        for block_index in range(self._num_conv_layers):\n",
    "            block_number = block_index + 1\n",
    "            layer_index = 0\n",
    "            self.layer2id['decoder']['conv_transpose{}'.format(block_number)] = 3 * block_index + layer_index \n",
    "            self.decoder.add_module(\n",
    "                'conv_transpose{}'.format(block_number),\n",
    "                self._add_conv_transpose_layer(block_index))\n",
    "            layer_index += 1\n",
    "            self.layer2id['decoder']['relu{}'.format(block_number)] = 3 * block_index + layer_index \n",
    "            self.decoder.add_module(\n",
    "                'relu{}'.format(block_number),\n",
    "                nn.ReLU())\n",
    "            layer_index += 1\n",
    "            self.layer2id['decoder']['batchnorm{}'.format(block_number)] = 3 * block_index + layer_index \n",
    "            self.decoder.add_module(\n",
    "                'batchnorm{}'.format(block_number),\n",
    "                nn.BatchNorm2d(self.conv_transpose_filters[block_index]))\n",
    "\n",
    "    def _add_conv_transpose_layer(self, block_index):\n",
    "        \"\"\"Add a convolutional transpose block to a graph of layers, consisting of\n",
    "        conv 2d + ReLU + batch normalization.\n",
    "        \"\"\"\n",
    "        out_channels = self.conv_transpose_filters\n",
    "        in_channels = []\n",
    "        in_channels.append(self.output_shape[len(conv_filters) - 1][0])\n",
    "        for i in range(len(self.conv_transpose_filters) - 1):\n",
    "            in_channels.append(self.conv_transpose_filters[i])\n",
    "        stride = self.conv_strides[::-1][block_index]\n",
    "        output_padding = 1 if stride > 1 else 0\n",
    "        \n",
    "        #print('conv_transpose in channels ', in_channels)\n",
    "        #print('conv_transpose out channels ', out_channels)  \n",
    "        \n",
    "        conv_transpose_layer = nn.ConvTranspose2d(\n",
    "            in_channels[block_index],\n",
    "            out_channels[block_index],\n",
    "            self.conv_kernels[::-1][block_index],\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            output_padding=output_padding\n",
    "        )\n",
    "        return conv_transpose_layer\n",
    "    \n",
    "    def _add_bottleneck(self):\n",
    "        \"\"\"Flatten encoder last conv layer output data\n",
    "           and add bottleneck to latent dimension\n",
    "        \"\"\"\n",
    "        self.shape_before_bottleneck = self.output_shape[self._num_conv_layers - 1]\n",
    "        self.encoder.add_module('bottleneck_flatten', nn.Flatten(1))\n",
    "        self.encoder.add_module('bottleneck_linear', \n",
    "                                nn.Linear(np.prod(self.shape_before_bottleneck), self.latent_space_dim * 2))\n",
    "\n",
    "    def _invert_bottleneck(self):\n",
    "        \"\"\"Reshape data from bottleneck latent dimension \n",
    "           to dimension of the output data after the encoder last conv layer\n",
    "        \"\"\"\n",
    "        self.decoder.add_module('bottleneck_linear', \n",
    "                                nn.Linear(self.latent_space_dim, np.prod(self.shape_before_bottleneck)))\n",
    "        self.decoder.add_module('bottleneck_unflatten', nn.Unflatten(1, self.shape_before_bottleneck))\n",
    "                                \n",
    "    def _get_encoder_layer(self, name):\n",
    "        return self.encoder[self.layer2id['encoder'][name]]\n",
    "    \n",
    "    def _get_encoder_layers(self):\n",
    "        return self.encoder\n",
    "        \n",
    "    def _get_decoder_layer(self, name):\n",
    "        return self.decoder[self.layer2id['decoder'][name]]\n",
    "        \n",
    "    def _get_decoder_layers(self):\n",
    "        return self.decoder\n",
    "    \n",
    "    def _get_output_shape(self, block_index):\n",
    "        return self.output_shape[block_index]\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.new_empty(std.size()).normal_()\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_logvar = self.encoder(x).view(-1, 2, self.latent_space_dim)\n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "n_mfcc = 40\n",
    "n_samples = 44\n",
    "batch_size = 16\n",
    "\n",
    "x = torch.rand(batch_size, 1, n_mfcc, n_samples)\n",
    "\n",
    "input_shape=(x.shape[1:])\n",
    "conv_filters=[32, 64, 64, 64]\n",
    "conv_kernels=[3, 3, 3, 3]\n",
    "conv_strides=[1, 2, 2, 1]\n",
    "latent_space_dim = 2\n",
    "    \n",
    "model = VAE(input_shape,\n",
    "              conv_filters,\n",
    "              conv_kernels,\n",
    "              conv_strides,\n",
    "              latent_space_dim)\n",
    "\n",
    "#print(model._get_encoder_layer('conv2'))\n",
    "#print(model._get_decoder_layer('conv_transpose3'))\n",
    "\n",
    "print(model._get_encoder_layers())\n",
    "print(model._get_decoder_layers())\n",
    "\n",
    "#for i in range(len(conv_filters)):\n",
    "#    print(np.prod(model._get_output_shape(i)))\n",
    "    \n",
    "#print(model.conv_filters)\n",
    "#print(model.conv_transpose_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27bd48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "y = model(x)\n",
    "print(y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24981bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [1, 2, 4, 8]\n",
    "\n",
    "layer2id['encoder'] = dict()\n",
    "layer2id['decoder'] = dict()\n",
    "layer_idx = conv_idx = 0\n",
    "\n",
    "for k in range(len(channels)-1):\n",
    "    layer2id['encoder']['conv{}'.format(conv_idx)] = layer_idx\n",
    "    print(layer2id['encoder']['conv0'])\n",
    "    layer_idx += 1\n",
    "    conv_idx += 1\n",
    "        \n",
    "#new_dic = {}\n",
    "#new_dic['encoder'] = {}\n",
    "#new_dic[1] = {}\n",
    "#new_dic['encoder'][2] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer2id['encoder']['conv1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c189e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can use this formula [(W−K+2P)/S]+1.\n",
    "\n",
    "#W is the input volume - in your case 128\n",
    "#K is the Kernel size - in your case 5\n",
    "#P is the padding - in your case 0 i believe\n",
    "#S is the stride - which you have not provided.\n",
    "\n",
    "n_mfcc = 40\n",
    "n_samples = 44\n",
    "\n",
    "input_shape=(n_mfcc, n_samples, 1)\n",
    "conv_filters=(32, 64, 64, 64)\n",
    "conv_kernels=(3, 3, 3, 3)\n",
    "conv_strides=(1, 2, 2, 1)\n",
    "\n",
    "print(type(conv_kernels))\n",
    "\n",
    "inp = input_shape\n",
    "for k in range(len(conv_kernels)):\n",
    "    K = conv_kernels[k]\n",
    "    S = conv_strides[k]\n",
    "    P = 1\n",
    "    out = []\n",
    "    for w in range(len(inp) -1):\n",
    "        W = inp[w]\n",
    "        out.append(int(((W - K + (2 * P)) / S) + 1))\n",
    "    out.append(conv_filters[k])\n",
    "    inp = out\n",
    "    \n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 40\n",
    "n_samples = 44\n",
    "\n",
    "input_shape=[n_mfcc, n_samples, 1]\n",
    "conv_filters=[32, 64, 64, 64]\n",
    "\n",
    "out_channels = conv_filters\n",
    "in_channels = []\n",
    "in_channels.append(input_shape[-1])\n",
    "for i in range(len(conv_filters) - 1):\n",
    "    in_channels.append(conv_filters[i])\n",
    "\n",
    "print(out_channels)\n",
    "print(in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = a[:-1][::-1]\n",
    "b.append(1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 10, 11)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "print(x.shape[1:])\n",
    "\n",
    "bottleneck = nn.Sequential(\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(np.prod(x.shape[1:]), 2),\n",
    ")\n",
    "\n",
    "inv_bottleneck = nn.Sequential(\n",
    "    nn.Linear(2, np.prod(x.shape[1:])),\n",
    "    nn.Unflatten(1, x.shape[1:])\n",
    ")\n",
    "\n",
    "enc = bottleneck(x)\n",
    "dec = inv_bottleneck(enc)\n",
    "\n",
    "print(enc.shape)\n",
    "print(dec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d521cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(10, 11, 64)\n",
    "#input = input[None, :]\n",
    "print(input.view(-1,np.prod(input.shape)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 11, 64)\n",
    "print(x.shape)\n",
    "print(x.unsqueeze(-1).shape)\n",
    "print(torch.unsqueeze(x, 0).shape)\n",
    "print(torch.unsqueeze(x, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fdf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(16, 1, 40, 666)\n",
    "\n",
    "conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "conv3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "conv4 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "convt1 = nn.ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "convt2 = nn.ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) \n",
    "convt3 = nn.ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "# Compute padding, output padding and dilation lists (x, y) in order to force \n",
    "# last ConvTranspose layer output to match first Conv layer input \n",
    "# need to solve the system given by the two formulas (given stride = (2, 2) and kernel = (3, 3) for last layer):\n",
    "# H_{out} = (H_{in} - 1) x stride[0] - 2 x padding[0] + dilation[0] x (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "# W_{out} = (W_{in} - 1) x stride[1] - 2 x padding[1] + dilation[1] x (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "\n",
    "\n",
    "def compute_padding(n_mfcc, dim_in, dim_out, stride, kernel):\n",
    "    exit = False\n",
    "    \n",
    "    stuff = range(n_mfcc)\n",
    "    stuff_list = []\n",
    "    \n",
    "    for subset in itertools.product(stuff, repeat=3):\n",
    "        stuff_list.append(subset)\n",
    "        \n",
    "    for padding, output_padding, dilation in sorted(stuff_list, key=sum): \n",
    "        if (float((dim_in - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1).is_integer() and\n",
    "            dilation > 0 and\n",
    "            dim_out == (dim_in - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1):\n",
    "            print(dim_out)\n",
    "            print((dim_in - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1)\n",
    "            break\n",
    "    \n",
    "    return padding, output_padding, dilation\n",
    "    \n",
    "\n",
    "y = conv4(conv3(conv2(conv1(x))))\n",
    "\n",
    "H_in = convt3(convt2(convt1(y))).shape[-2]\n",
    "W_in = convt3(convt2(convt1(y))).shape[-1]\n",
    "H_out = x.shape[-2]\n",
    "W_out = x.shape[-1]\n",
    "stride = (2, 2)\n",
    "kernel = (3, 3)\n",
    "n_mfcc = 40\n",
    "\n",
    "padding_0, output_padding_0, dilation_0 = compute_padding(n_mfcc, H_in, H_out, stride[0], kernel[0])\n",
    "padding_1, output_padding_1, dilation_1 = compute_padding(n_mfcc, W_in, W_out, stride[1], kernel[1])\n",
    "\n",
    "print(padding_0) \n",
    "print(padding_1)\n",
    "print(output_padding_0) \n",
    "print(output_padding_1)\n",
    "print(dilation_0) \n",
    "print(dilation_1)\n",
    "\n",
    "convt4 = nn.ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), \n",
    "                            padding=(padding_0, padding_1), \n",
    "                            output_padding=(output_padding_0, output_padding_1), \n",
    "                            dilation=(dilation_0, dilation_1))\n",
    "\n",
    "print(conv1(x).shape)\n",
    "print(conv2(conv1(x)).shape)\n",
    "print(conv3(conv2(conv1(x))).shape)\n",
    "print(conv4(conv3(conv2(conv1(x)))).shape)\n",
    "\n",
    "print(\"- - - - - - - - - - - - - - -\")\n",
    "\n",
    "print(convt1(y).shape)\n",
    "print(convt2(convt1(y)).shape)\n",
    "print(convt3(convt2(convt1(y))).shape)\n",
    "print(convt4(convt3(convt2(convt1(y)))).shape)\n",
    "\n",
    "print(\"- - - - - - - - - - - - - - -\")\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    conv1,\n",
    "    conv2,\n",
    "    conv3,\n",
    "    conv4\n",
    ")\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    convt1,\n",
    "    convt2,\n",
    "    convt3,\n",
    "    convt4\n",
    ")\n",
    "\n",
    "enc = encoder(x)\n",
    "dec = decoder(enc)\n",
    "\n",
    "print(enc.shape)\n",
    "print(dec.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "194a1073",
   "metadata": {},
   "source": [
    "Input: (N, C_{in}, H_{in}, W_{in}) or (C_{in}, H_{in}, W_{in})\n",
    "Output: (N, C_{out}, H_{out}, W_{out}) or (C_{out}, H_{out}, W_{out})\n",
    "\n",
    "where\n",
    "\n",
    "H_{out} = (H_{in} - 1) x stride[0] - 2 x padding[0] + dilation[0] x (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "W_{out} = (W_{in} - 1) x stride[1] - 2 x padding[1] + dilation[1] x (kernel_size[1] - 1) + output_padding[1] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb906d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = range(40)\n",
    "stuff_list = []\n",
    "\n",
    "for subset in itertools.product(stuff, repeat=3):\n",
    "    stuff_list.append(subset)\n",
    "    \n",
    "print(sorted(stuff_list, key=sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3.0\n",
    "\n",
    "print(3 == num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filters = [32, 64, 64, 64]\n",
    "input_shape = [1, 40, 130]\n",
    "\n",
    "enc_input_channels = conv_filters[:-1]\n",
    "enc_input_channels.insert(0, input_shape[0])\n",
    "enc_output_channels = conv_filters\n",
    "dec_input_channels = enc_output_channels[::-1]\n",
    "dec_output_channels = enc_input_channels[::-1]\n",
    "\n",
    "print(\"input_channel (encoder) = \", enc_input_channels)\n",
    "print(\"output_channel (encoder) = \", enc_output_channels)\n",
    "print(\"input_channel (decoder) = \", dec_input_channels)\n",
    "print(\"output_channel (decoder) = \", dec_output_channels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
